---
title: "rbmi Advanced Functionality"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{advanced}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
)
```


## Introduction

The purpose of this vignette is to provide an overview of some of the more advanced
features that the package offers. Please note that this vignette
will not cover any of the underlying statistical theory. 


In order to demonstrate these advanced functions we will first create a simulated dataset
and perform a mock analysis so that we have all the require objects available to us.

```{r}
library(rbmi)
library(dplyr)
set.seed(169)
dat_full <- simulate_data(n = 120) %>% as_tibble()

## Introduce missingness
dat <- dat_full
missing_index_vis2 <- rbinom(nrow(dat), 1, 0.3) == 1 & dat$visit == "visit_2"
missing_index_vis3 <- rbinom(nrow(dat), 1, 0.4) == 1 & dat$visit == "visit_3"
dat[missing_index_vis2, "outcome"] <- NA_real_
dat[missing_index_vis3, "outcome"] <- NA_real_

# create data_ice setting the imputation method to MAR for
# each patient with at least one missing value
dat_ice <- dat %>%
    arrange(id, visit) %>%
    filter(is.na(outcome)) %>%
    group_by(id) %>%
    slice(1) %>%
    ungroup() %>%
    select(id, visit) %>%
    mutate(strategy = "MAR")


# Define the names of key variables in our dataset using `set_vars()`
# Note that covariates argument can contain interactions
vars <- set_vars(
    outcome = "outcome",
    visit = "visit",
    subjid = "id",
    group = "group",
    covariates = c("age", "sex", "group*visit")
)


```


## Efficiently Changing Imputation Strategies

The `draws()` function is by far the most computationally intensive function in
rbmi. It is recognised that this can pose an issue for doing repeated analyses
using different imputation strategies. To try and address this issue `impute()`
has an additional argument `update_strategies` which allows the user to 
change each subjects imputation strategy without having to re-run the `draws()`
function.

Please note though that this functionality comes with a few key limitations:
- The visit on which the ICE occured can not be updated.
- The imputation strategy cannot be changed from MAR to a non-MAR strategy if .
there is any observed data after the ICE visit

Both of these restrictions exist to protect the validity of the imputation model.
It is worth noting that whilst a strategy of MAR cannot be changed to non-MAR, a
non-MAR strategy can be changed to MAR. However, please bear in mind that a potential 
issue with this
is that it means that the imputation model hasn't used all of the available data;
as such the package will issue a warning if the user attempts to do this. 

As an example, lets say in our first analysis we want to use the copy 
increments from reference strategy but then as a sensitivity we wanted to use the
jump to reference imputation strategy, this could be done as follows:

```{r}
dat_ice_CIR <- dat_ice
dat_ice_CIR$strategy <- "CIR"
dat_ice_CIR

draw_obj <- draws(
    data = dat,
    data_ice = dat_ice_CIR,
    vars = vars,
    method = method_condmean(n_samples = 15)
)

impute_obj_CIR <- impute(
    draw_obj,
    references = c("A" = "B", "B" = "B")
)
ana_obj_CIR <- analyse(
    impute_obj_CIR,
    vars = set_vars()
)
pool(ana_obj_CIR)

## Now we re-use our draws samples but using the Jump to Reference 
## Imputation strategy

dat_ice_JR <- dat_ice
dat_ice_JR$strategy <- "JR"
dat_ice_JR

impute_obj_JR <- impute(
    draw_obj,
    references = c("A" = "B", "B" = "B"),
    update_strategy = dat_ice_JR
)
ana_obj_JR <- analyse(
    impute_obj_JR,
    vars = set_vars()
)
pool(ana_obj_JR)
```


## Custom Strategy Functions

Out of the box rbmi has support for the following imputation strategies:

- Missing at Random (MAR)
- Jump to Reference (JR)
- Copy Reference (CR)
- Copy Increments from Reference (CIR)
- Last Mean Carried Forward (LMCF)

However, rbmi also supports the ability for users to define their own imputation 
strategies. To do this there are 3 things the user needs to do:

1. Define their own imputation strategy function.
2. Specify which patients use this strategy in the `data_ice` dataset provided to `draws()`.
3. Provide the imputation strategy function to `impute()`.

This is perhaps better demonstrated by an example. We will implement a strategy
in which all post-ice missing values are be imputed by the average outcome between 
the subject's group and reference group.

To do this we first need to define our imputation function. This function must take
3 arguments: `pars_group`, `pars_ref` and `index_mar`.  Both `pars_group` and `pars_ref`
will be a list with elements `mu` (a numeric vector) and `sigma` (a covariance matrix)
which are the parameters for the patient's outcome distribution
assuming that they belong to their own group and the reference group respectively.
The `index_mar` argument
is then a logical vector indicating which visits for the subject meet the MAR assumption.
This can be used to identify the observations that occur after an ICE that are addressed
with a non-MAR imputation strategy. 

As such our example imputation function could be coded as:


```{r}
strategy_AVG <- function(pars_group, pars_ref, index_mar) {
    mean_xy <- function(x, y) mean(c(x, y))
    mu_mean <- mapply(
        mean_xy,
        pars_group$mu,
        pars_ref$mu,
        SIMPLIFY = T
    )
    x <- pars_group
    x$mu[!index_mar] <- mu_mean[!index_mar]
    return(x)
}
```

And an example showing its use:
```{r}
pars_group <- list(
    mu = c(1, 2, 3),
    sigma = as_vcov(c(1, 3, 2), c(0.4, 0.5, 0.45))
)

pars_ref <- list(
    mu = c(5, 6, 7),
    sigma = as_vcov(c(2, 1, 1), c(0.7, 0.8, 0.5))
)

index_mar <- c(TRUE, TRUE, FALSE)

strategy_AVG(pars_group, pars_ref, index_mar)
```

To incorporate this into rbmi, `data_ice` needs to be updated such that subjects are 
specified as using this `AVG` imputation strategy. Additionally the function then needs
to be provided to `impute()` via the `getStrategies()` function as shown below:

```{r}
dat_ice$strategy <- "AVG"
dat_ice

draw_obj <- draws(
    data = dat,
    data_ice = dat_ice,
    vars = vars,
    method = method_condmean(n_samples = 10)
)

impute_obj <- impute(
    draw_obj,
    references = c("A" = "B", "B" = "B"),
    strategies = getStrategies(AVG = strategy_AVG)
)
```



## Custom Analysis Functions

By default rbmi will analyse the data by performing ancova at each visit independently,
returning the "treatment effect" estimate as well as the corresponding least square means
for each group. If the user wants to perform a different analysis, or return different
statistics from the analysis, then this can be done by using a custom analysis function.

The rules for this custom analysis function are that it must take a `data.frame` as its 
first argument and that it must return a named `list` with each element itself being a `list`
containing a single numeric element called `est`. If you had originally specified `method_bayes()` 
or `method_approxbayes()` then the analysis function must also return a standard error estimate 
for the given statistics and the degrees of freedom of the complete-data analysis model. These
should be included in the named `list` using names `se` and `df` respectively.

The reason for this is that if `method_bayes()` or `method_approxbayes()` are specified
then `pool()` will perform its inference based upon Rubin's rules which require the estimate (`est`),
the standard error of the estimate (`se`) as well as the degrees of freedom (`df`).


As an example lets say that for your analysis
you are also interested in calculating the overall mean at each visit (i.e. not the treatment
effect), this could then be done as follows:

```{r}
mean_fun <- function(dat, ...) {

    mod <- dat %>%
        group_by(visit) %>%
        summarise(m = mean(outcome))

    results <- lapply(
        mod$m,
        function(x) list(est = x)
    )

    names(results) <- paste0("mean_", mod$visit)
    return(results)
}
```

This analysis function can then be used in combination with `analyse()` as follows:

```{r}
anl_obj <- analyse(
    imputations = impute_obj_CIR,
    fun = mean_fun
)

pool(anl_obj)
```



## Delta Adjustment

The `delta` argument of `analyse()` allows users to modify the outcome variable
which can be utilised as part of a tipping point or sensitivity analysis. To do 
this the user needs to provide a `data.frame` containing a column for the subject
and visit which identifies the observation to be adjusted, and then a 3rd column
called `delta` which specifies how much to offset the outcome by.

The `delta_template()` function creates a skeleton `data.frame` containing
1 row per subject per visit with the value of delta set to 0 for all observations.
The `delta_template()` function though also has two additional arguments `delta`
and `dlag` which allows the user to specify initial accumulative delta values based upon a
default value and a scaling coefficient based upon how far away the visit
in question is from the ICE visit. 

More specifically; the `delta` argument specifies the default amount of delta
that should be applied to each visit (if it was both post-ICE & unobserved), whilst
`dlag` specifies the scaling coefficient to be applied based upon the visits proximity
to the ICE visit. This is perhaps best illustrated with an example:

Let `delta = c(5,6,7,8)` and `dlag=c(1,2,3,4)` (i.e. assuming there are 4 visits) and lets 
say that the subject had an ICE on visit 2. The calculation would then be as follows:

```
v1  v2  v3  v4
--------------
 5   6   7   8  # delta assigned to each visit
 0   1   2   3  # scaling starting from the first visit after the subjects ICE
--------------
 0   6  14  24  # delta * scaling
--------------
 0   6  20  44  # accumulative sum / delta to be applied to each visit
```

That is to say the subject would have a delta offset of 0 applied for visit-1, 6 for 
visit-2, 20 for visit-3 and 44 for visit-4. As a comparison, lets say that the subject 
instead had their ICE on visit 3, the calculation would then be as follows:

```
v1  v2  v3  v4
--------------
 5   6   7   8  # delta assigned to each visit
 0   0   1   2  # scaling starting from the first visit after the subjects ICE
--------------
 0   0   7  16  # delta * scaling
--------------
 0   0   7  23  # accumulative sum / delta to be applied to each visit
 ```
 
In terms of practical usage, lets say that you wanted a delta of 5 to be used for all
post ICE visits regardless of their proximity to the ICE visit. This can be achieved
by setting `delta = c(5,5,5,5)` and `dlag = c(1,0,0,0)`. For example lets say a subject
had their ICE on visit-1, then the calculation would be as follows:

```
v1  v2  v3  v4
--------------
 5   5   5   5  # delta assigned to each visit
 1   0   0   0  # scaling starting from the first visit after the subjects ICE
--------------
 5   0   0  0  # delta * scaling
--------------
 5   5   5  5  # accumulative sum / delta to be applied to each visit
 ```
 
Another way of using these arguments is to set delta to be the difference in time 
between visits and dlag to be the amount of delta per unit of time. For example 
lets say that we have a visit on weeks 1, 5, 6 & 9 and that we want a delta of 3
to be applied for each week after an ICE. This can be achieved by setting 
`delta = c(0,4,1,3)` (the difference in weeks between each visit) and `dlag = c(3, 3, 3, 3)`. 
For example lets say we have a subject who had their ICE on week-5 (i.e. visit-2) then 
the calculation would be:

```
v1  v2  v3  v4
--------------
 0   4   1   3  # delta assigned to each visit
 0   0   3   3  # scaling starting from the first visit after the subjects ICE
--------------
 0   0   3   9  # delta * scaling
--------------
 0   0   3  12  # accumulative sum / delta to be applied to each visit
```

i.e. on week-6 (1 week after the ICE) they have a delta of 3 and on week-9 (4 weeks
after the ICE) they have a delta of 12


To show this in action, lets say that we want a constant delta of 5 to be applied (regardless of
the lag) to all unobserved post-ICE visits in the treatment arm only. This can be achieved as follows:

First use the `delta` and `dlag` arguments of `delta_template()` to setup a template `data.frame`
in which every unobserved post-ICE visit has a delta of 5:
```{r}
delta_df <- delta_template(
    impute_obj_CIR, 
    delta = c(5, 5, 5), 
    dlag = c(1, 0, 0)
)

as_tibble(delta_df)
```

Next we can use the additional metadata variables provided by `delta_template()` to  manually 
reset the delta values for the control group back to 0:
```{r}
delta_df2 <- delta_df %>% 
    mutate(delta = if_else(group == "B", 0, delta))

as_tibble(delta_df2)
```

Finally we can now use our delta `data.frame` to apply our desired delta offset to our analysis:
```{r}
anl_delta <- analyse(impute_obj_CIR, delta = delta_df2, vars = set_vars())
pool(anl_delta)  
```


Please note that in the case of observed post-ICE data, `delta_template()` will calculate the delta for all
observations assuming that all post-ICE data are missing; it will then set the value of `delta` to
0 for those observations that were actually observed. If you wish to retain the value of `delta`
for the observed post-ICE visits then you can use the `missing_only = FALSE` argument to do so I.e.:

```{r}
delta_template(
    impute_obj_CIR, 
    delta = c(5, 5, 5), 
    dlag = c(1, 0, 0),
    missing_only = FALSE
) %>% 
    as_tibble()
```
