---
title: "rbmi Quickstart"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{quickstart}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
)
```

## Introduction

The purpose of this vignette is to provide a 15 minute quickstart guide to the rbmi
package showcasing it's primary functions and usage. Please note that this vignette
will not cover any of the underlying theory. 

The rbmi package consists of 4 core functions as well as a several key helper functions.
In particular the core functions are:

- `draws()` - Creates the imputation models
- `impute()` - Creates multiple imputed datasets
- `analyse()` - Analyses the multiple imputed datasets
- `pool()` - Combines the multiple analysis results into a single statistic

## The Data

In order to demonstrate the package we will use a publicly available example data set from an antidepressant clinical trial of an active drug versus placebo. The relevant endpoint is the Hamilton 17-item rating scale for depression (HAMD17) which was assessed at baseline and weeks 1, 2, 4, and 6. Study drug discontinuation occurred in 24% subjects from the active drug and 26% subjects from placebo. All data after study drug discontinuation are missing and there is a single additional intermittent missing observation.

```{r}
library(rbmi)
library(dplyr)

data("antidepressant_data")
dat <- antidepressant_data
```

We consider an imputation model with the mean change from baseline in the HAMD17 score as the outcome (variable `CHANGE` in the dataset), included the treatment group (`THERAPY`), the (categorical) visit (`VISIT`), treatment-by-visit interactions, the baseline HAMD17 score (`BASVAL`), and baseline HAMD17-by-visit interactions as covariates, and assumed a common unstructured covariance matrix in both groups. The chosen analysis model is ANCOVA which adjusts for the baseline HAMD17 value.

`rbmi` expects its input dataset to be complete; that is that there must be 1 row
per patient per visit. Missing outcome values should be coded as `NA`, while missing values in the covariates are not allowed. If your dataset is incomplete then the `expand_locf()` helper function can be used to add in any missing rows, using LOCF imputation to impute the covariate values. In our dataset the rows corresponding to missing outcomes are not present, to address this we will therefore use the `expand_locf()` function
as follows:

```{r}

## Use expand_locf to add those dropped observations into the dataset
dat <- expand_locf(
    dat,
    PATIENT = levels(dat$PATIENT), # expand by PATIENT and VISIT 
    VISIT = levels(dat$VISIT),
    vars = c("BASVAL", "THERAPY"), # fill with LOCF BASVAL and THERAPY
    group = c("PATIENT"),
    order = c("PATIENT", "VISIT")
)
```


## Draws

The draws function is used to create our imputation models. The 3 main inputs to the `draws()`
function include:

- `data` the primary longitudinal data.frame containing the outcome variable and all covariates
- `data_ice` a data.frame specifying which visit (if any) the patient's intercurrent 
event (ICE) occurred on, or more precisely the first visit in which the outcome has been affected by the ICE. If the patient had multiple ICEs this should 
specify the first visit affected by the ICE which is to be imputed by a non-MAR. It also 
specifies which reference based imputation strategy we want to use.
- `method` specifies what method we want to use to fit our imputation models as well as what
method we want to use to generate our imputed values. 

In our example the patients ICE visit is the
first visit in which a missing value has occurred. We assume that all patients will be
imputed under the Jump To Reference (JR) strategy. We will create 150 imputation models using Bayesian
methods to sample the model coefficients from their posterior distributions for a model 
of `CHANGE ~ 1 + BASVAL * VISIT + THERAPY * VISIT`.

```{r}
# create data_ice setting the imputation method to JR for
# each patient with at least one missing value
dat_ice <- dat %>% 
    arrange(PATIENT, VISIT) %>% 
    filter(is.na(CHANGE)) %>% 
    group_by(PATIENT) %>% 
    slice(1) %>%
    ungroup() %>% 
    select(PATIENT, VISIT) %>% 
    mutate(strategy = "JR")

# The patient with id 3618 is the unique one that has an intermittent missing values ->
# remove from data_ice since he does not experience any ICE.
# (it will be automatically imputed under MAR assumption)
dat_ice <- dat_ice[-which(dat_ice$PATIENT == 3618),]

dat_ice

# Define the names of key variables in our dataset using `set_vars()`
# Note that covariates argument can contain interactions
vars <- set_vars(
    outcome = "CHANGE",
    visit = "VISIT",
    subjid = "PATIENT",
    group = "THERAPY",
    covariates = c("BASVAL*VISIT", "THERAPY*VISIT")
)

# Define what method we want to use e.g. here we specify we 
# want to use Bayesian methods to create 100 samples
method <- method_bayes(
    burn_in = 200,
    burn_between = 5,
    n_samples = 150,
    verbose = FALSE
)


# Create samples for the parameters of interest by running the 
# draws() function
set.seed(987)
drawObj <- draws(
    data = dat,
    data_ice = dat_ice,
    vars = vars,
    method = method
)
drawObj
```


Note the use of `set_vars()` which specifies what the names of the key variables 
within the dataset. Additionally please note that whilst `vars$group` and `vars$visit`
are added as terms to the imputation model by default that their interaction is not,
thus the inclusion of `group * visit` in the list of covariates. 

Available methods include:

- Bayes - `method_bayes()`
- Approximate Bayes - `method_approxbayes()`
- Conditional Mean (Bootstrap) - `method_condmean(type = "bootstrap")`
- Conditional Mean (Jackknife) - `method_condmean(type = "jackknife")`

Available imputation strategies include:

- Missing Completely at Random - "MAR"
- Jump to Reference - "JR"
- Copy Reference - "CR"
- Copy increments from Reference - "CIR"
- Last Mean Carried Forward - "LMCF"


## Impute

The next step is to use our imputation models to generate our imputed datasets. This is
done via the `impute()` function. The function only has 2 key inputs; the imputation 
models and the references to use when imputing the missing values. It's usage is thus:

```{r}
imputeObj <- impute(
    drawObj,
    c("DRUG" = "PLACEBO", "PLACEBO" = "PLACEBO")
)
imputeObj
```

In this instance we are specifying that group `PLACEBO` should use itself as its reference group and that group `DRUG` should
use the group `PLACEBO` as its reference group (as standard for imputation using reference-based methods). 

Generally speaking, there is no need to see or directly interact with any of the imputed
datasets. However if you do wish to inspect them they can be extracted from the imputation
object using the `extract_imputed_dfs()` helper function, i.e.:

```{r}

imputed_dfs <- extract_imputed_dfs(imputeObj)
imputed_dfs[[1]]
imputed_dfs[[2]]
```

Note that in the case of `method_condmean()` the first imputed dataset will always be the
"full" dataset containing all the original patients. In the case of `type=jackknife` the
other datasets will all contain n-1 patients, whilst in the case of `type=bootstrap` each
dataset will contain a bootstrap sample of patients.


## Analyse

The next step is to run our analysis on each of our imputed datasets. This is done by defining
an analysis function and then getting `analyse()` to map this function across each of our
imputed datasets. For this vignette will we use the `ancova()` function provided by the rbmi
package which fits an ANCOVA model independently for each visit returning the treatment 
effect and least square means for each visit. 

```{r}
anaObj <- analyse(
    imputeObj,
    ancova,
    vars = set_vars(
        subjid = "PATIENT",
        outcome = "CHANGE",
        visit = "VISIT",
        group = "THERAPY",
        covariates = c("BASVAL")
    )
)
anaObj
```

Note that, similar to `draws()`, the `ancova()` function needs to use the `set_vars()`
function to tell it the names of the key variables within the data as well as which
covariates we want to include in our analysis model.

Additionally we can use the `delta` argument of `analyse()` to perform a delta adjustment. 
Essentially this works by specifying a data.frame that contains the amount
of bias to add to the outcome variable. This must be specified separately for each patient
at each visit. This data.frame must contain the columns `vars$visit`, `vars$subjid` and `delta`.

It is appreciated that this is potentially tedious to specify and as such some helper
functions have been provided to simplify this process. In particular
`delta_template()` provides a shell data.frame where the bias is set to 0 for all
patients but where many additional meta-variables are provided in order to support
us defining our own logic.  

For example lets say we want to add a bias of 5 to all
missing observations in the control arm. That could then be implemented as follows:

```{r}
## For reference show the additional meta variables provided
delta_template(imputeObj) %>% as_tibble()

delta_df <- delta_template(imputeObj) %>%
    as_tibble() %>% 
    mutate(delta = if_else(THERAPY == "PLACEBO" & is_missing , 5, 0)) %>% 
    select(PATIENT, VISIT, delta)
    
delta_df

anaObj_delta <- analyse(
    imputeObj,
    ancova,
    delta = delta_df,
    vars = set_vars(
        subjid = "PATIENT",
        outcome = "CHANGE",
        visit = "VISIT",
        group = "THERAPY",
        covariates = c("BASVAL")
    )
)
```

## Pool

Finally, the `pool()` function can be used to summarise all of our analysis results down 
into a single statistic including standard error, confidence intervals and a p-value for
the hypothesis test that $H_0 = 0$.

Note that the pooling method is automatically derived based on the method that was specified
in the original call to `draws()`:

- If `method_bayes()` or `method_approxbayes()` was used then pooling and inference are based on Rubin's rules
- If `method_condmean(type = "bootstrap") ` was used then inference is based on either normal approximation of the treatment effect distribution (`pool(..., type = "normal")`) or on percentiles (`pool(..., type = "percentile")`)
- If `method_condmean(type = "jackknife")` was used then inference is based on normal approximation

Since we have used Bayesian methods in this vignette the `pool()` function will automatically use Rubin's rules.


```{r}
poolObj <- pool(
    anaObj, 
    conf.level = 0.95, 
    alternative = "two.sided"
)
poolObj
```

The table of values shown in the print message can be extracted using the `as.data.frame()`
function on the pool object i.e.:

```{r}
as.data.frame(poolObj)
```
